[
    {
        "Name": "adaptive_block_size",
        "Title": "Adaptive Block Size: Dynamic Context Window Adjustment for Efficient Training",
        "Experiment": "Modify the model to dynamically adjust its block size during training, starting with a smaller block size and gradually increasing it. This could potentially lead to faster initial training and better long-range dependency learning.",
        "Interestingness": 6,
        "Feasibility": 4,
        "Novelty": 4
    },
    {
        "Name": "layerwise_learning_rates",
        "Title": "Layer-wise Learning Rate Adaptation: Optimizing Training Dynamics in Transformer Models",
        "Experiment": "Implement layer-wise learning rates, where each transformer layer has its own learning rate. Modify the configure_optimizers function to assign different learning rates to different layers, with deeper layers having lower learning rates. Compare the training dynamics, convergence speed, and final performance with the baseline model.",
        "Interestingness": 4,
        "Feasibility": 6,
        "Novelty": 2
    },
    {
        "Name": "spectral_normalization",
        "Title": "Spectral Normalization for Improved Generalization in Small Language Models",
        "Experiment": "Implement spectral normalization in the Linear layers of the GPT model. Modify the _init_weights function to apply spectral normalization to the weight matrices after initialization. Use the spectral_norm function from torch.nn.utils. Train the model on the shakespeare_char, enwik8, and text8 datasets, and evaluate the validation loss and average inference speed. Compare the results with the baseline model to assess the impact of spectral normalization on generalization and training stability.",
        "Interestingness": 7,
        "Feasibility": 6,
        "Novelty": 5
    },
    {
        "Name": "mixup_augmentation",
        "Title": "Mixup Data Augmentation for Enhanced Generalization in Small Language Models",
        "Experiment": "Implement mixup data augmentation in the get_batch function. Modify the get_batch function to: 1. Sample two batches of data (X1, Y1) and (X2, Y2). 2. Sample a lambda value from a Beta distribution with parameter alpha (e.g., alpha=0.2). 3. Create mixed inputs X_mixed = lambda * X1 + (1 - lambda) * X2. 4. Create mixed targets Y_mixed = lambda * Y1 + (1 - lambda) * Y2. Note that this is character-level training, therefore the targets are class indices, so Y_mixed needs to be handled as a weighted average of one-hot encoded vectors, and the loss needs to be adapted. Specifically, instead of using F.cross_entropy, compute the weighted cross-entropy loss manually. Train the model with mixup enabled and compare the validation loss and generation quality with the baseline model. Experiment with different values of alpha (e.g., 0.1, 0.2, 0.4) to find the optimal mixup strength. Adapt the final info dictionary to output the alpha parameter of the mixup augmentation. The experiment should output final info, training log, and validation log.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6
    },
    {
        "Name": "gradient_noise_injection",
        "Title": "Gradient Noise Injection for Enhanced Generalization in Small Language Models",
        "Experiment": "Implement gradient noise injection in the training loop. Modify the training loop to: 1. After the backward pass (scaler.scale(loss).backward()), before clipping the gradients, inject Gaussian noise into the gradients of the model parameters. The noise should be sampled from a Gaussian distribution with mean 0 and a standard deviation that is proportional to the current learning rate (e.g., std = learning_rate * noise_scale). Experiment with different values of noise_scale (e.g., 0.01, 0.05, 0.1). 2. Clip the gradient. Train the model with gradient noise injection enabled and compare the validation loss and generation quality with the baseline model. Adapt the final info dictionary to output the noise_scale parameter of the gradient noise injection. The experiment should output final info, training log, and validation log.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 6
    }
]